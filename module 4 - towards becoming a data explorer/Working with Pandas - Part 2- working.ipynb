{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "As previously stated, pandas provides extensive functionalities for exploratory data analysis. It is often used in conjunction with other libraries like **NumPy, Matplotlib and Seaborn**. Thus, it is common practice to import all these upfront before data loading and exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with tabular data\n",
    "\n",
    "Pandas provides functionalities to load tabular data from different file formats such as **.txt, .csv, .xls, .xlsx,** etc. \n",
    "\n",
    "**.txt** and **.csv** can be loaded using the **pandas.read_csv()** function:\n",
    "<code>\n",
    "pandas.read_csv(filepath, sep= sep, delimiter=None, header='infer', names=None, index_col=None, usecols=None, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, , thousands=None, decimal='.') </code>\n",
    "\n",
    "A similar function exists for reading **.xls**, and  **.xlsx,** files.\n",
    "\n",
    "<code>\n",
    "pandas.read_excel(filepath, sheet_name=0, header=0, names=None, index_col=None, usecols=None, squeeze=False, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, parse_dates=False, date_parser=None, thousands=None, comment=None, skipfooter=0, convert_float=True, mangle_dupe_cols=True, storage_options=None)\n",
    "</code>\n",
    "\n",
    "To illustrate, we would start by loading the boston_structured.txt file. But first, I need to extract the column names using using string operations (kindly revisit module 1 for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the boston_structured.txt file, read the lines and extract the lines corresponding to the column names\n",
    "file = open('boston_structured.txt')\n",
    "lines = file.readlines()\n",
    "col_names = [((lines[index]).split())[0] for index in range(7,21)]\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now load the data and use col_names for column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at 5 random sample of the data and save it as sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be sorted with the index as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Data Description and Statistics\n",
    "Pandas provides the info() and describe() methods for generating quick information and descriptive statistics about our data. Let's try each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing data \n",
    "\n",
    "It is common to have some missing data when working with actual data. These are identified as NaN in dataframe and must be remove or replaced before working with the data. This is known as data imputation. The following link is included for you t o read more about the different techniques used for data imputing https://en.wikipedia.org/wiki/Imputation_(statistics). The simplest technique is to remove missing data. This can be done using the **pandas.DataFrame.dropna()**. You can read more about this function here https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html. To check if our data contains any missing values, we would use the **isnull() and isnull().sum() methods** as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values with isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values with isnull.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data imputation vs drop() function\n",
    "\n",
    "Sometimes, the number of missing values in a specific column may be some much that removing the column may be a better solution than data imputing. The drop() method is designed for this. You can read more about this function here https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html \n",
    "\n",
    "Now, when missing values are removed, they sometimes cause the index to be disorganized. To reset the index, the **reset_index()** function is applied. Let's use the **TSLA.xlsx** file to illustrate these points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and view the file TSLA.xlsx here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view first 10 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see some statistics use percentiles = [.05,0.25, 0.5, 0.75, .95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see some missing values in our dataset. Let's use mean imputation to fix all missing values. To do so, let's first make a copy of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check for missing values in data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean imputation in data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check for missing values again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if our index is in order "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Going forward**\n",
    "\n",
    "What if we choose to drop rows with missing values instead? What will happen to the index? Let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make another copy of data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check for missing values in data_copy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use dropna() on data_copy1 & check for missing values again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check out the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly that the index is no longer in order and as such should be reset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check out the index again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that the column **Adj Close** has the highest number of missing values. For a small dataset, using dropna() will further reduce the overall number of rows, reducing the number of data points we would have to work with. In such a case, it may be a lot better to drop the column(s) with high number of missing values instead. Let's illustrate how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take another copy of data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check for missing values in data_copy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop Adj Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at some sample of data_copy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check for missing values in data_copy2 again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "While the drop() function has removed the column with the highest number of missing values, it does not fix the missing values in else where. To fix those, imputation or dropna() is required afterward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Filtering and Masking\n",
    "Assuming we are required to filter the dataset to show only points for which **Open** price is at least **500.0**. We can define a mask on the **Open** column to achieve this.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a mask for Open >= 500.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply multiple masks using Open >= 500.0 and Close >= 550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with dates\n",
    "Assuming we are required to filter the data for a given year or a specific month in each year, we would first need to extract these information from the **Date** column. **Pandas.DatetimeIndex()** is a good functionality to achieve this. \n",
    "\n",
    "To illustrate, let's say we wish to filter our dataset to only year 2020. To do that, we would need to extract the years from the **Date** column and then apply mask as we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's extract the years and add them as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at some samples of data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now mask the dataset to year == 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting data to Excel\n",
    "With the addition of the year column to our dataset, we may consider exporting the new data to Excel as a csv or xlsx file. The simple syntax to do that is\n",
    "\n",
    "<code> dataframe_name.to_csv(filename.csv)  # this saves a .csv file for us </code>\n",
    "\n",
    "<code> dataframe_name.to_excel(filename.xlsx)  # this saves a .xlsx file for us </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's export data_copy to excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing DataFrames\n",
    "In addition to the possibility of using matplotlib.pyplot to visualize specific information from our dataframe, pandas also offers functionalities to directly generate different customizable visuals (using matplotlib backend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the Open column against Date using plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, use .plot() function in pandas to plot both Open & Close against Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the column names in data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bar plots of Open, High, Low and Close columns against Date for the year 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Relationship Between each Pair of Parameters\n",
    "It is important to check the relationship between each pair of variables in our dataset. This allows us to identify highly correlating (or collinear) features which can potential cause issues with our model. Pairplot() and heatmap() functions in seaborn are usefully for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make the pairplots for Open, High, Low and Close for year 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make the heatmap for the spearman's correlation coeffficients for the variables for the year 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "There are quite a lot of things that we could talk about in this aspect. However, the information I have provided in these module is more than sufficient to explore any dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
